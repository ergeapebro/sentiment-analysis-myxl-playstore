{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd7985b5-47e2-4e02-8154-240b2a240ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\TUF\n",
      "[nltk_data]     Gaming\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\TUF\n",
      "[nltk_data]     Gaming\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\TUF\n",
      "[nltk_data]     Gaming\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Dropout, Dense, Masking\n",
    "\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from indoNLP.preprocessing import replace_slang\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5081f30-1b6c-4e33-bb30-b7cf0b8dee0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>appVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4c669b18-5f47-4ef6-8824-681f545daf15</td>\n",
       "      <td>Gie Ozora</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/ACg8oc...</td>\n",
       "      <td>tidak rekomen, signal hanya 1 bar,giliran lebi...</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>8.0.0</td>\n",
       "      <td>2025-03-26 10:38:20</td>\n",
       "      <td>Hi Kak, maaf ya, agar dapat dibantu cek lebih ...</td>\n",
       "      <td>2025-03-26 12:00:40</td>\n",
       "      <td>8.0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>376d73cb-f14e-4c97-b7dc-549fcc4d2222</td>\n",
       "      <td>Abdul Syafii</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>aduh makin paraha ajah sih ini makin kesini ma...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8.1.0</td>\n",
       "      <td>2025-04-01 20:12:28</td>\n",
       "      <td>Hi Kak, Maya mohon maaf ya atas kendalanya, yu...</td>\n",
       "      <td>2025-04-01 22:01:11</td>\n",
       "      <td>8.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d733f9a9-5de4-4ce2-81f4-d13d75930812</td>\n",
       "      <td>Rabun 26</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/ACg8oc...</td>\n",
       "      <td>Aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "      <td>4</td>\n",
       "      <td>956</td>\n",
       "      <td>8.0.0</td>\n",
       "      <td>2025-03-10 00:54:48</td>\n",
       "      <td>Hi Kak, terima kasih atas ulasannya. Apabila a...</td>\n",
       "      <td>2025-03-10 02:01:06</td>\n",
       "      <td>8.0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4e4ebdb1-2a06-474a-8863-512f8c9c4947</td>\n",
       "      <td>Puji Nugroho</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>aplikasi sering error, harus login dengan nomo...</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>8.1.0</td>\n",
       "      <td>2025-03-27 20:30:20</td>\n",
       "      <td>Hi Kak, mohon maaf ya atas ketidaknyamanan yan...</td>\n",
       "      <td>2025-03-27 22:01:07</td>\n",
       "      <td>8.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3e317d3f-be0f-47fd-8aee-600f56487ef2</td>\n",
       "      <td>Elyse</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>Tolong untuk pihak XL jika update aplikasi itu...</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "      <td>8.0.0</td>\n",
       "      <td>2025-03-15 19:12:28</td>\n",
       "      <td>Hi Kak, terima kasih atas kepercayaannya untuk...</td>\n",
       "      <td>2024-05-22 10:01:04</td>\n",
       "      <td>8.0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId      userName  \\\n",
       "0  4c669b18-5f47-4ef6-8824-681f545daf15     Gie Ozora   \n",
       "1  376d73cb-f14e-4c97-b7dc-549fcc4d2222  Abdul Syafii   \n",
       "2  d733f9a9-5de4-4ce2-81f4-d13d75930812      Rabun 26   \n",
       "3  4e4ebdb1-2a06-474a-8863-512f8c9c4947  Puji Nugroho   \n",
       "4  3e317d3f-be0f-47fd-8aee-600f56487ef2         Elyse   \n",
       "\n",
       "                                           userImage  \\\n",
       "0  https://play-lh.googleusercontent.com/a/ACg8oc...   \n",
       "1  https://play-lh.googleusercontent.com/a-/ALV-U...   \n",
       "2  https://play-lh.googleusercontent.com/a/ACg8oc...   \n",
       "3  https://play-lh.googleusercontent.com/a-/ALV-U...   \n",
       "4  https://play-lh.googleusercontent.com/a-/ALV-U...   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \\\n",
       "0  tidak rekomen, signal hanya 1 bar,giliran lebi...      1             69   \n",
       "1  aduh makin paraha ajah sih ini makin kesini ma...      2              7   \n",
       "2  Aplikasi nya sering bermasalah jaringan selalu...      4            956   \n",
       "3  aplikasi sering error, harus login dengan nomo...      1             32   \n",
       "4  Tolong untuk pihak XL jika update aplikasi itu...      1            129   \n",
       "\n",
       "  reviewCreatedVersion                   at  \\\n",
       "0                8.0.0  2025-03-26 10:38:20   \n",
       "1                8.1.0  2025-04-01 20:12:28   \n",
       "2                8.0.0  2025-03-10 00:54:48   \n",
       "3                8.1.0  2025-03-27 20:30:20   \n",
       "4                8.0.0  2025-03-15 19:12:28   \n",
       "\n",
       "                                        replyContent            repliedAt  \\\n",
       "0  Hi Kak, maaf ya, agar dapat dibantu cek lebih ...  2025-03-26 12:00:40   \n",
       "1  Hi Kak, Maya mohon maaf ya atas kendalanya, yu...  2025-04-01 22:01:11   \n",
       "2  Hi Kak, terima kasih atas ulasannya. Apabila a...  2025-03-10 02:01:06   \n",
       "3  Hi Kak, mohon maaf ya atas ketidaknyamanan yan...  2025-03-27 22:01:07   \n",
       "4  Hi Kak, terima kasih atas kepercayaannya untuk...  2024-05-22 10:01:04   \n",
       "\n",
       "  appVersion  \n",
       "0      8.0.0  \n",
       "1      8.1.0  \n",
       "2      8.0.0  \n",
       "3      8.1.0  \n",
       "4      8.0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('myxl_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9514b5-3d43-4fcb-9aef-7ec960b2470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   reviewId              20000 non-null  object\n",
      " 1   userName              20000 non-null  object\n",
      " 2   userImage             20000 non-null  object\n",
      " 3   content               20000 non-null  object\n",
      " 4   score                 20000 non-null  int64 \n",
      " 5   thumbsUpCount         20000 non-null  int64 \n",
      " 6   reviewCreatedVersion  17905 non-null  object\n",
      " 7   at                    20000 non-null  object\n",
      " 8   replyContent          19091 non-null  object\n",
      " 9   repliedAt             19091 non-null  object\n",
      " 10  appVersion            17905 non-null  object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f95a44-b63e-4651-9282-6d738c60eb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewId                   0\n",
      "userName                   0\n",
      "userImage                  0\n",
      "content                    0\n",
      "score                      0\n",
      "thumbsUpCount              0\n",
      "reviewCreatedVersion    2095\n",
      "at                         0\n",
      "replyContent             909\n",
      "repliedAt                909\n",
      "appVersion              2095\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565b4a3f-9edd-4b07-97e3-0c2ca6c18cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef45805-6d5f-43a9-9f7e-e5f60b0d6b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tidak rekomen, signal hanya 1 bar,giliran lebi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aduh makin paraha ajah sih ini makin kesini ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aplikasi sering error, harus login dengan nomo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tolong untuk pihak XL jika update aplikasi itu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  tidak rekomen, signal hanya 1 bar,giliran lebi...\n",
       "1  aduh makin paraha ajah sih ini makin kesini ma...\n",
       "2  Aplikasi nya sering bermasalah jaringan selalu...\n",
       "3  aplikasi sering error, harus login dengan nomo...\n",
       "4  Tolong untuk pihak XL jika update aplikasi itu..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['reviewId','userName','userImage','score','thumbsUpCount','reviewCreatedVersion','at','replyContent','repliedAt','appVersion'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e82043f-3a47-481f-a636-a35e2be14bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # menghapus link\n",
    "    text = re.sub(r'[0-9]+', '', text) # menghapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # menghapus karakter selain huruf dan angka\n",
    "    text = re.sub(r'[!$%^&*@#()_+|~=`{}\\[\\]%\\-:\";\\'<>?,.\\/]', ' ', text) # hapus simbol\n",
    "    # remove koreksi duplikasi tiga karakter beruntun atau lebih (contoh. yukkk)\n",
    "    text = re.sub(r'([a-zA-Z])\\1\\1','\\\\1', text)\n",
    "    # remove ASCII dan unicode\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "    text = re.sub(r'^[ ]|[ ]$','', text) # remove spasi di awal dan akhir kalimat\n",
    "    text= re.sub(r'\\s+', ' ', text).strip() # remove whitespaces\n",
    "    text = re.sub(' +', ' ', text) # remove spasi ganda (atau lebih) menjadi satu spasi\n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks\n",
    "    return text\n",
    "\n",
    "def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "def fix_slangwords(text):\n",
    "    try:\n",
    "        text = normalize_text(text)\n",
    "        return replace_slang(text)\n",
    "    except KeyError:\n",
    "        return text\n",
    "        \n",
    "def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def filteringText(text): # Menghapus stopwords dalam teks\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    listStopwords.update(listStopwords1)\n",
    "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\"])\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    "\n",
    "def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata\n",
    "    # Membuat objek stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    # Memecah teks menjadi daftar kata\n",
    "    words = text.split()\n",
    "    # Menerapkan stemming pada setiap kata dalam daftar\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    # Menggabungkan kata-kata yang telah distem\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    return stemmed_text\n",
    "\n",
    "def toSentence(list_words): # Mengubah daftar kata menjadi kalimat\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3e6c6a-1a48-4c3d-b462-54e0db80075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membersihkan teks dan menyimpannya di kolom 'text_clean'\n",
    "df['text_clean'] = df['content'].apply(cleaningText)\n",
    "\n",
    "# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'\n",
    "df['text_casefoldingText'] = df['text_clean'].apply(casefoldingText)\n",
    "\n",
    "df['text_normalize'] = df['text_casefoldingText'].apply(normalize_text)\n",
    "\n",
    "# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'\n",
    "df['text_slangwords'] = df['text_normalize'].apply(fix_slangwords)\n",
    "\n",
    "# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'\n",
    "df['text_tokenizingText'] = df['text_slangwords'].apply(tokenizingText)\n",
    "\n",
    "# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'\n",
    "df['text_stopword'] = df['text_tokenizingText'].apply(filteringText)\n",
    "\n",
    "# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'\n",
    "df['text_akhir'] = df['text_stopword'].apply(toSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc03c5f7-346c-4720-b1e1-473ff98699ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_casefoldingText</th>\n",
       "      <th>text_normalize</th>\n",
       "      <th>text_slangwords</th>\n",
       "      <th>text_tokenizingText</th>\n",
       "      <th>text_stopword</th>\n",
       "      <th>text_akhir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>Sudah update, app sering tidak merespon, kadan...</td>\n",
       "      <td>Sudah update app sering tidak merespon kadang ...</td>\n",
       "      <td>sudah update app sering tidak merespon kadang ...</td>\n",
       "      <td>sudah update app sering tidak merespon kadang ...</td>\n",
       "      <td>sudah update app sering tidak merespon kadang ...</td>\n",
       "      <td>[sudah, update, app, sering, tidak, merespon, ...</td>\n",
       "      <td>[update, app, merespon, kadang, lihat, sisa, k...</td>\n",
       "      <td>update app merespon kadang lihat sisa kuota in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>aplikasinya sii bagus dan sangat membantu tapi...</td>\n",
       "      <td>aplikasinya sii bagus dan sangat membantu tapi...</td>\n",
       "      <td>aplikasinya sii bagus dan sangat membantu tapi...</td>\n",
       "      <td>aplikasinya sii bagus dan sangat membantu tapi...</td>\n",
       "      <td>aplikasinya sih bagus dan sangat membantu tapi...</td>\n",
       "      <td>[aplikasinya, sih, bagus, dan, sangat, membant...</td>\n",
       "      <td>[aplikasinya, bagus, membantu, sinyalnya, lho,...</td>\n",
       "      <td>aplikasinya bagus membantu sinyalnya lho jelek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>to the point aja ngasih alasannya. kenapa gak ...</td>\n",
       "      <td>to the point aja ngasih alasannya kenapa gak m...</td>\n",
       "      <td>to the point aja ngasih alasannya kenapa gak m...</td>\n",
       "      <td>to the point aja ngasih alasannya kenapa gak m...</td>\n",
       "      <td>tapi the point saja mengasih alasannya kenapa ...</td>\n",
       "      <td>[tapi, the, point, saja, mengasih, alasannya, ...</td>\n",
       "      <td>[point, mengasih, alasannya, login, masukkan, ...</td>\n",
       "      <td>point mengasih alasannya login masukkan kode r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>Saya kasih bintang 1 karena aplikasi XL di say...</td>\n",
       "      <td>Saya kasih bintang karena aplikasi XL di saya ...</td>\n",
       "      <td>saya kasih bintang karena aplikasi xl di saya ...</td>\n",
       "      <td>saya kasih bintang karena aplikasi xl di saya ...</td>\n",
       "      <td>saya kasih bintang karena aplikasi xl di saya ...</td>\n",
       "      <td>[saya, kasih, bintang, karena, aplikasi, xl, d...</td>\n",
       "      <td>[kasih, bintang, aplikasi, xl, mahal, isi, pul...</td>\n",
       "      <td>kasih bintang aplikasi xl mahal isi pulsa maha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>Jaringan tidak stabil patah patah untuk bermai...</td>\n",
       "      <td>Jaringan tidak stabil patah patah untuk bermai...</td>\n",
       "      <td>jaringan tidak stabil patah patah untuk bermai...</td>\n",
       "      <td>jaringan tidak stabil patah patah untuk bermai...</td>\n",
       "      <td>jaringan tidak stabil patah patah untuk bermai...</td>\n",
       "      <td>[jaringan, tidak, stabil, patah, patah, untuk,...</td>\n",
       "      <td>[jaringan, stabil, patah, patah, bermain, mobi...</td>\n",
       "      <td>jaringan stabil patah patah bermain mobile leg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  \\\n",
       "19995  Sudah update, app sering tidak merespon, kadan...   \n",
       "19996  aplikasinya sii bagus dan sangat membantu tapi...   \n",
       "19997  to the point aja ngasih alasannya. kenapa gak ...   \n",
       "19998  Saya kasih bintang 1 karena aplikasi XL di say...   \n",
       "19999  Jaringan tidak stabil patah patah untuk bermai...   \n",
       "\n",
       "                                              text_clean  \\\n",
       "19995  Sudah update app sering tidak merespon kadang ...   \n",
       "19996  aplikasinya sii bagus dan sangat membantu tapi...   \n",
       "19997  to the point aja ngasih alasannya kenapa gak m...   \n",
       "19998  Saya kasih bintang karena aplikasi XL di saya ...   \n",
       "19999  Jaringan tidak stabil patah patah untuk bermai...   \n",
       "\n",
       "                                    text_casefoldingText  \\\n",
       "19995  sudah update app sering tidak merespon kadang ...   \n",
       "19996  aplikasinya sii bagus dan sangat membantu tapi...   \n",
       "19997  to the point aja ngasih alasannya kenapa gak m...   \n",
       "19998  saya kasih bintang karena aplikasi xl di saya ...   \n",
       "19999  jaringan tidak stabil patah patah untuk bermai...   \n",
       "\n",
       "                                          text_normalize  \\\n",
       "19995  sudah update app sering tidak merespon kadang ...   \n",
       "19996  aplikasinya sii bagus dan sangat membantu tapi...   \n",
       "19997  to the point aja ngasih alasannya kenapa gak m...   \n",
       "19998  saya kasih bintang karena aplikasi xl di saya ...   \n",
       "19999  jaringan tidak stabil patah patah untuk bermai...   \n",
       "\n",
       "                                         text_slangwords  \\\n",
       "19995  sudah update app sering tidak merespon kadang ...   \n",
       "19996  aplikasinya sih bagus dan sangat membantu tapi...   \n",
       "19997  tapi the point saja mengasih alasannya kenapa ...   \n",
       "19998  saya kasih bintang karena aplikasi xl di saya ...   \n",
       "19999  jaringan tidak stabil patah patah untuk bermai...   \n",
       "\n",
       "                                     text_tokenizingText  \\\n",
       "19995  [sudah, update, app, sering, tidak, merespon, ...   \n",
       "19996  [aplikasinya, sih, bagus, dan, sangat, membant...   \n",
       "19997  [tapi, the, point, saja, mengasih, alasannya, ...   \n",
       "19998  [saya, kasih, bintang, karena, aplikasi, xl, d...   \n",
       "19999  [jaringan, tidak, stabil, patah, patah, untuk,...   \n",
       "\n",
       "                                           text_stopword  \\\n",
       "19995  [update, app, merespon, kadang, lihat, sisa, k...   \n",
       "19996  [aplikasinya, bagus, membantu, sinyalnya, lho,...   \n",
       "19997  [point, mengasih, alasannya, login, masukkan, ...   \n",
       "19998  [kasih, bintang, aplikasi, xl, mahal, isi, pul...   \n",
       "19999  [jaringan, stabil, patah, patah, bermain, mobi...   \n",
       "\n",
       "                                              text_akhir  \n",
       "19995  update app merespon kadang lihat sisa kuota in...  \n",
       "19996  aplikasinya bagus membantu sinyalnya lho jelek...  \n",
       "19997  point mengasih alasannya login masukkan kode r...  \n",
       "19998  kasih bintang aplikasi xl mahal isi pulsa maha...  \n",
       "19999  jaringan stabil patah patah bermain mobile leg...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "441796b6-2a72-4808-ae0e-ee77f1d718fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   content               20000 non-null  object\n",
      " 1   text_clean            20000 non-null  object\n",
      " 2   text_casefoldingText  20000 non-null  object\n",
      " 3   text_normalize        20000 non-null  object\n",
      " 4   text_slangwords       20000 non-null  object\n",
      " 5   text_tokenizingText   20000 non-null  object\n",
      " 6   text_stopword         20000 non-null  object\n",
      " 7   text_akhir            20000 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02091423-9e1d-42a3-a09d-4970f2e6fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04789972-46ff-4cd7-bb6e-bcc5403fd9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12a6a019-efde-4c14-bf34-c7f583c96e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model dan tokenizer\n",
    "model_name = \"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a1173bc-5c4b-476b-88a0-f0c8f447ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping label (disesuaikan dari metadata model)\n",
    "id2label = {\n",
    "    0: \"negative\",\n",
    "    1: \"neutral\",\n",
    "    2: \"positive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a13e518-7853-4763-8a9a-2cb7ceeb2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi prediksi\n",
    "def predict_sentiment(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"neutral\"  # atau label default lain, misal \"netral\"\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(torch.long) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        if outputs.logits.shape[1] == 0:\n",
    "            return \"neutral\"\n",
    "\n",
    "        predicted_class_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "        return id2label[predicted_class_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3c7e100-0821-4f85-a215-7fb834dd4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled = df_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d73ee0b-ba0b-48d3-b275-41244a626ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Terapkan ke kolom DataFrame\n",
    "df_labeled['label'] = df_labeled['text_akhir'].apply(predict_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a77146d-4105-461d-8173-b69d6b085cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_casefoldingText</th>\n",
       "      <th>text_normalize</th>\n",
       "      <th>text_slangwords</th>\n",
       "      <th>text_tokenizingText</th>\n",
       "      <th>text_stopword</th>\n",
       "      <th>text_akhir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tidak rekomen, signal hanya 1 bar,giliran lebi...</td>\n",
       "      <td>tidak rekomen signal hanya bargiliran lebih da...</td>\n",
       "      <td>tidak rekomen signal hanya bargiliran lebih da...</td>\n",
       "      <td>tidak rekomen signal hanya bargiliran lebih da...</td>\n",
       "      <td>tidak rekomen signal hanya bargiliran lebih da...</td>\n",
       "      <td>[tidak, rekomen, signal, hanya, bargiliran, le...</td>\n",
       "      <td>[rekomen, signal, bargiliran, bar, koneksi, in...</td>\n",
       "      <td>rekomen signal bargiliran bar koneksi internet...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aduh makin paraha ajah sih ini makin kesini ma...</td>\n",
       "      <td>aduh makin paraha ajah sih ini makin kesini ma...</td>\n",
       "      <td>aduh makin paraha ajah sih ini makin kesini ma...</td>\n",
       "      <td>aduh makin paraha ajah sih ini makin kesini ma...</td>\n",
       "      <td>aduh makin paraha saja sih ini makin kesini ma...</td>\n",
       "      <td>[aduh, makin, paraha, saja, sih, ini, makin, k...</td>\n",
       "      <td>[aduh, paraha, kesini, rekomen, signal, bar, g...</td>\n",
       "      <td>aduh paraha kesini rekomen signal bar giliran ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "      <td>Aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "      <td>aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "      <td>aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "      <td>aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "      <td>[aplikasi, nya, sering, bermasalah, jaringan, ...</td>\n",
       "      <td>[aplikasi, bermasalah, jaringan, lag, bermain,...</td>\n",
       "      <td>aplikasi bermasalah jaringan lag bermain game ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aplikasi sering error, harus login dengan nomo...</td>\n",
       "      <td>aplikasi sering error harus login dengan nomor...</td>\n",
       "      <td>aplikasi sering error harus login dengan nomor...</td>\n",
       "      <td>aplikasi sering error harus login dengan nomor...</td>\n",
       "      <td>aplikasi sering error harus login dengan nomor...</td>\n",
       "      <td>[aplikasi, sering, error, harus, login, dengan...</td>\n",
       "      <td>[aplikasi, error, login, nomor, server, eror, ...</td>\n",
       "      <td>aplikasi error login nomor server eror lihat n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tolong untuk pihak XL jika update aplikasi itu...</td>\n",
       "      <td>Tolong untuk pihak XL jika update aplikasi itu...</td>\n",
       "      <td>tolong untuk pihak xl jika update aplikasi itu...</td>\n",
       "      <td>tolong untuk pihak xl jika update aplikasi itu...</td>\n",
       "      <td>tolong untuk pihak xl jika update aplikasi itu...</td>\n",
       "      <td>[tolong, untuk, pihak, xl, jika, update, aplik...</td>\n",
       "      <td>[tolong, xl, update, aplikasi, perbaiki, kemba...</td>\n",
       "      <td>tolong xl update aplikasi perbaiki kembangin b...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  tidak rekomen, signal hanya 1 bar,giliran lebi...   \n",
       "1  aduh makin paraha ajah sih ini makin kesini ma...   \n",
       "2  Aplikasi nya sering bermasalah jaringan selalu...   \n",
       "3  aplikasi sering error, harus login dengan nomo...   \n",
       "4  Tolong untuk pihak XL jika update aplikasi itu...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  tidak rekomen signal hanya bargiliran lebih da...   \n",
       "1  aduh makin paraha ajah sih ini makin kesini ma...   \n",
       "2  Aplikasi nya sering bermasalah jaringan selalu...   \n",
       "3  aplikasi sering error harus login dengan nomor...   \n",
       "4  Tolong untuk pihak XL jika update aplikasi itu...   \n",
       "\n",
       "                                text_casefoldingText  \\\n",
       "0  tidak rekomen signal hanya bargiliran lebih da...   \n",
       "1  aduh makin paraha ajah sih ini makin kesini ma...   \n",
       "2  aplikasi nya sering bermasalah jaringan selalu...   \n",
       "3  aplikasi sering error harus login dengan nomor...   \n",
       "4  tolong untuk pihak xl jika update aplikasi itu...   \n",
       "\n",
       "                                      text_normalize  \\\n",
       "0  tidak rekomen signal hanya bargiliran lebih da...   \n",
       "1  aduh makin paraha ajah sih ini makin kesini ma...   \n",
       "2  aplikasi nya sering bermasalah jaringan selalu...   \n",
       "3  aplikasi sering error harus login dengan nomor...   \n",
       "4  tolong untuk pihak xl jika update aplikasi itu...   \n",
       "\n",
       "                                     text_slangwords  \\\n",
       "0  tidak rekomen signal hanya bargiliran lebih da...   \n",
       "1  aduh makin paraha saja sih ini makin kesini ma...   \n",
       "2  aplikasi nya sering bermasalah jaringan selalu...   \n",
       "3  aplikasi sering error harus login dengan nomor...   \n",
       "4  tolong untuk pihak xl jika update aplikasi itu...   \n",
       "\n",
       "                                 text_tokenizingText  \\\n",
       "0  [tidak, rekomen, signal, hanya, bargiliran, le...   \n",
       "1  [aduh, makin, paraha, saja, sih, ini, makin, k...   \n",
       "2  [aplikasi, nya, sering, bermasalah, jaringan, ...   \n",
       "3  [aplikasi, sering, error, harus, login, dengan...   \n",
       "4  [tolong, untuk, pihak, xl, jika, update, aplik...   \n",
       "\n",
       "                                       text_stopword  \\\n",
       "0  [rekomen, signal, bargiliran, bar, koneksi, in...   \n",
       "1  [aduh, paraha, kesini, rekomen, signal, bar, g...   \n",
       "2  [aplikasi, bermasalah, jaringan, lag, bermain,...   \n",
       "3  [aplikasi, error, login, nomor, server, eror, ...   \n",
       "4  [tolong, xl, update, aplikasi, perbaiki, kemba...   \n",
       "\n",
       "                                          text_akhir     label  \n",
       "0  rekomen signal bargiliran bar koneksi internet...  positive  \n",
       "1  aduh paraha kesini rekomen signal bar giliran ...  positive  \n",
       "2  aplikasi bermasalah jaringan lag bermain game ...  positive  \n",
       "3  aplikasi error login nomor server eror lihat n...  positive  \n",
       "4  tolong xl update aplikasi perbaiki kembangin b...   neutral  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cc9fe80-e595-4a18-a6a7-345a6ab4e2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "positive    11339\n",
      "neutral      5791\n",
      "negative     2870\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_labeled[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e26a6-ce31-46d8-a42b-2ddfaa38818d",
   "metadata": {},
   "source": [
    "## Word embedding - fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54fa8686-e517-44b5-8591-ef0abf7674f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_labeled['text_akhir']\n",
    "y = df_labeled['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5e1d6bb-c915-43c3-8b52-806035bcf521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data ke bentuk label dan teks\n",
    "train_data = '\\n'.join([f\"__label__{label} {text}\" for label, text in zip(y_train, X_train)])\n",
    "test_data = '\\n'.join([f\"__label__{label} {text}\" for label, text in zip(y_test, X_test)])\n",
    "\n",
    "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(train_data)\n",
    "\n",
    "with open(\"test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb0647e8-c706-4069-8aee-8a10d1c7806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "model_ft = fasttext.train_supervised(input=\"train.txt\", epoch=25, lr=1.0, wordNgrams=2, verbose=2, minCount=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c985292-ba2b-4c38-af5a-d0d126006644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 0.80075, 0.80075)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.test(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90ab8c60-7dbc-4888-813a-d3986b7f1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    label = model_ft.predict(text)[0][0]\n",
    "    return label.replace(\"__label__\", \"\")\n",
    "\n",
    "df['predicted_label'] = df['text_akhir'].apply(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d610e9f1-b6c0-46db-9bd9-b66ef0442511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dapatkan dimensi vektor dari kata apa pun yang ada di model\n",
    "sample_word = list(model_ft.get_words())[0]\n",
    "vector_dim = len(model_ft.get_word_vector(sample_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c386d0fa-030e-4cc6-8a8a-3755a28b831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil semua kata dari FastText\n",
    "def vectorize_text(text, model_ft, vector_dim):\n",
    "    tokens = text.split()\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vectors.append(model_ft.get_word_vector(token))\n",
    "        except:\n",
    "            vectors.append(np.zeros(vector_dim))\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bccca8ec-aac7-441e-9bd1-259e30e10321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubah teks menjadi vektor\n",
    "X_vectors = df_labeled['text_akhir'].apply(lambda x: vectorize_text(str(x), model_ft, vector_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ba8c446-36ef-43b1-83a9-48a2cc6c57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "max_len = max(X_vectors.apply(len))\n",
    "X_padded = pad_sequences(X_vectors, maxlen=max_len, padding='post', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e13caf4c-15db-4509-8d56-894083c111ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode label\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df_labeled['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e08ef71-797c-47fc-8a31-61aeb0f8c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63d7c2ff-eb34-429e-a8f0-53c3a599fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode label\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc52aa-2f8c-4c94-bb21-8f3f54dd9271",
   "metadata": {},
   "source": [
    "## Model deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69e5b55f-566b-4c92-9844-88516a9d2495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TUF Gaming\\miniconda3\\envs\\main-da\\lib\\site-packages\\keras\\src\\layers\\core\\masking.py:48: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Masking(mask_value=0., input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model_lstm.add(LSTM(128, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.4))\n",
    "model_lstm.add(LSTM(64, return_sequences=False))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(64, activation='relu'))\n",
    "model_lstm.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a423a66-a4b3-462a-97b1-3ada6fd4f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "275fa240-ab57-40e9-a180-63c1ad1dadf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">92</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">92</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">92</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ masking (\u001b[38;5;33mMasking\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m92\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m92\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m117,248\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m92\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m49,408\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m195\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171,011</span> (668.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m171,011\u001b[0m (668.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171,011</span> (668.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m171,011\u001b[0m (668.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tampilkan arsitektur model\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58d04805-d803-4081-9a73-9d48c74f3f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 65ms/step - accuracy: 0.8152 - loss: 0.5058 - val_accuracy: 0.8675 - val_loss: 0.3253\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.8609 - loss: 0.3550 - val_accuracy: 0.8692 - val_loss: 0.3261\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 67ms/step - accuracy: 0.8657 - loss: 0.3397 - val_accuracy: 0.8737 - val_loss: 0.3121\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - accuracy: 0.8666 - loss: 0.3314 - val_accuracy: 0.8745 - val_loss: 0.3152\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - accuracy: 0.8717 - loss: 0.3213 - val_accuracy: 0.8798 - val_loss: 0.3066\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 70ms/step - accuracy: 0.8723 - loss: 0.3121 - val_accuracy: 0.8805 - val_loss: 0.2996\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - accuracy: 0.8738 - loss: 0.3068 - val_accuracy: 0.8875 - val_loss: 0.2970\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - accuracy: 0.8854 - loss: 0.2942 - val_accuracy: 0.8792 - val_loss: 0.3052\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - accuracy: 0.8792 - loss: 0.3003 - val_accuracy: 0.8867 - val_loss: 0.2871\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - accuracy: 0.8869 - loss: 0.2913 - val_accuracy: 0.8875 - val_loss: 0.2896\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model_lstm.fit(\n",
    "    X_train, \n",
    "    y_train_cat, \n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    epochs=10, \n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dccb75f9-5fc1-4afd-bfa3-eb5fc5f4fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.8841 - loss: 0.3022\n",
      "Test Loss: 0.2896 | Test Accuracy: 0.8875\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "loss, accuracy = model_lstm.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Loss: {loss:.4f} | Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544c2aa-115d-4951-852d-d3d877e409d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1eacd53-4ce4-478c-beeb-9d7530cc0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build GRU model\n",
    "model_gru = Sequential()\n",
    "model_gru.add(Masking(mask_value=0., input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model_gru.add(GRU(128, return_sequences=True))\n",
    "model_gru.add(Dropout(0.4))\n",
    "model_gru.add(GRU(64, return_sequences=False))\n",
    "model_gru.add(Dropout(0.3))\n",
    "model_gru.add(Dense(64, activation='relu'))\n",
    "model_gru.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0eac2300-e874-4095-8375-6ec7d55488b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model_gru.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d1f0957-afa4-4a8e-9c1a-ccf46d0c0c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 65ms/step - accuracy: 0.7962 - loss: 0.5154 - val_accuracy: 0.8612 - val_loss: 0.3551\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 69ms/step - accuracy: 0.8588 - loss: 0.3548 - val_accuracy: 0.8730 - val_loss: 0.3188\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - accuracy: 0.8605 - loss: 0.3480 - val_accuracy: 0.8643 - val_loss: 0.3257\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 69ms/step - accuracy: 0.8675 - loss: 0.3310 - val_accuracy: 0.8773 - val_loss: 0.3111\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 72ms/step - accuracy: 0.8702 - loss: 0.3247 - val_accuracy: 0.8758 - val_loss: 0.3214\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 69ms/step - accuracy: 0.8695 - loss: 0.3282 - val_accuracy: 0.8820 - val_loss: 0.2975\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - accuracy: 0.8671 - loss: 0.3206 - val_accuracy: 0.8832 - val_loss: 0.2978\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 67ms/step - accuracy: 0.8778 - loss: 0.3095 - val_accuracy: 0.8815 - val_loss: 0.2984\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - accuracy: 0.8774 - loss: 0.3058 - val_accuracy: 0.8785 - val_loss: 0.2998\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - accuracy: 0.8769 - loss: 0.3048 - val_accuracy: 0.8783 - val_loss: 0.2989\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model_gru.fit(\n",
    "    X_train, \n",
    "    y_train_cat, \n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    epochs=10, \n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70e5fc5d-3eaa-4a18-a3aa-fe1c30f26e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8702 - loss: 0.3113\n",
      "Test Loss: 0.2989 | Test Accuracy: 0.8783\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "loss, accuracy = model_gru.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Loss: {loss:.4f} | Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398d94fe-cf53-46cf-a625-bdf60c5e621d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a99259c-28e1-44ee-a4a0-00d6e75023d1",
   "metadata": {},
   "source": [
    "## Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f31be33-a63a-4601-b642-1bbd57653e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c90053d-61e0-4e85-b16c-b86f9842d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Membaca data kamus kata-kata positif dari GitHub\n",
    "lexicon_positive = dict()\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
    "\n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_positive[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive\n",
    "else:\n",
    "    print(\"Failed to fetch positive lexicon data\")\n",
    "\n",
    "# Membaca data kamus kata-kata negatif dari GitHub\n",
    "lexicon_negative = dict()\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
    "\n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_negative[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative\n",
    "else:\n",
    "    print(\"Failed to fetch negative lexicon data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "436471aa-d77f-4f3d-80f8-b37f95f0e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menentukan polaritas sentimen dari tweet\n",
    "\n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    #for word in text:\n",
    "\n",
    "    score = 0\n",
    "    # Inisialisasi skor sentimen ke 0\n",
    "\n",
    "    for word in text:\n",
    "        # Mengulangi setiap kata dalam teks\n",
    "\n",
    "        if (word in lexicon_positive):\n",
    "            score = score + lexicon_positive[word]\n",
    "            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen\n",
    "\n",
    "    for word in text:\n",
    "        # Mengulangi setiap kata dalam teks (sekali lagi)\n",
    "\n",
    "        if (word in lexicon_negative):\n",
    "            score = score + lexicon_negative[word]\n",
    "            # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen\n",
    "\n",
    "    polarity=''\n",
    "    # Inisialisasi variabel polaritas\n",
    "\n",
    "    if (score > 0):\n",
    "        polarity = 'positive'\n",
    "        # Jika skor sentimen lebih besar atau sama dengan 0, maka polaritas adalah positif\n",
    "    elif (score < 0):\n",
    "        polarity = 'negative'\n",
    "        # Jika skor sentimen kurang dari 0, maka polaritas adalah negatif\n",
    "    else:\n",
    "      polarity = 'neutral'\n",
    "    # Ini adalah bagian yang bisa digunakan untuk menentukan polaritas netral jika diperlukan\n",
    "\n",
    "    return score, polarity\n",
    "    # Mengembalikan skor sentimen dan polaritas teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cca255a-2aba-42f6-9b58-7e355495087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity\n",
      "negative    13104\n",
      "positive     5973\n",
      "neutral       923\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "results = df_clean['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "df_clean['polarity_score'] = results[0]\n",
    "df_clean['polarity'] = results[1]\n",
    "print(df_clean['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b3db1ec-7d97-4171-b153-514b2e76e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   content               20000 non-null  object\n",
      " 1   text_clean            20000 non-null  object\n",
      " 2   text_casefoldingText  20000 non-null  object\n",
      " 3   text_normalize        20000 non-null  object\n",
      " 4   text_slangwords       20000 non-null  object\n",
      " 5   text_tokenizingText   20000 non-null  object\n",
      " 6   text_stopword         20000 non-null  object\n",
      " 7   text_akhir            20000 non-null  object\n",
      " 8   polarity_score        20000 non-null  int64 \n",
      " 9   polarity              20000 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72f10032-fd17-4f4c-982f-50d7f216310a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_casefoldingText</th>\n",
       "      <th>text_normalize</th>\n",
       "      <th>text_slangwords</th>\n",
       "      <th>text_tokenizingText</th>\n",
       "      <th>text_stopword</th>\n",
       "      <th>text_akhir</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tidak rekomen, signal hanya 1 bar,giliran lebi...</td>\n",
       "      <td>tidak rekomen signal hanya bargiliran lebih da...</td>\n",
       "      <td>tidak rekomen signal hanya bargiliran lebih da...</td>\n",
       "      <td>tidak rekomen signal hanya bargiliran lebih da...</td>\n",
       "      <td>tidak rekomen signal hanya bargiliran lebih da...</td>\n",
       "      <td>[tidak, rekomen, signal, hanya, bargiliran, le...</td>\n",
       "      <td>[rekomen, signal, bargiliran, bar, koneksi, in...</td>\n",
       "      <td>rekomen signal bargiliran bar koneksi internet...</td>\n",
       "      <td>-13</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aduh makin paraha ajah sih ini makin kesini ma...</td>\n",
       "      <td>aduh makin paraha ajah sih ini makin kesini ma...</td>\n",
       "      <td>aduh makin paraha ajah sih ini makin kesini ma...</td>\n",
       "      <td>aduh makin paraha ajah sih ini makin kesini ma...</td>\n",
       "      <td>aduh makin paraha saja sih ini makin kesini ma...</td>\n",
       "      <td>[aduh, makin, paraha, saja, sih, ini, makin, k...</td>\n",
       "      <td>[aduh, paraha, kesini, rekomen, signal, bar, g...</td>\n",
       "      <td>aduh paraha kesini rekomen signal bar giliran ...</td>\n",
       "      <td>-17</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "      <td>Aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "      <td>aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "      <td>aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "      <td>aplikasi nya sering bermasalah jaringan selalu...</td>\n",
       "      <td>[aplikasi, nya, sering, bermasalah, jaringan, ...</td>\n",
       "      <td>[aplikasi, bermasalah, jaringan, lag, bermain,...</td>\n",
       "      <td>aplikasi bermasalah jaringan lag bermain game ...</td>\n",
       "      <td>-12</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aplikasi sering error, harus login dengan nomo...</td>\n",
       "      <td>aplikasi sering error harus login dengan nomor...</td>\n",
       "      <td>aplikasi sering error harus login dengan nomor...</td>\n",
       "      <td>aplikasi sering error harus login dengan nomor...</td>\n",
       "      <td>aplikasi sering error harus login dengan nomor...</td>\n",
       "      <td>[aplikasi, sering, error, harus, login, dengan...</td>\n",
       "      <td>[aplikasi, error, login, nomor, server, eror, ...</td>\n",
       "      <td>aplikasi error login nomor server eror lihat n...</td>\n",
       "      <td>-19</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tolong untuk pihak XL jika update aplikasi itu...</td>\n",
       "      <td>Tolong untuk pihak XL jika update aplikasi itu...</td>\n",
       "      <td>tolong untuk pihak xl jika update aplikasi itu...</td>\n",
       "      <td>tolong untuk pihak xl jika update aplikasi itu...</td>\n",
       "      <td>tolong untuk pihak xl jika update aplikasi itu...</td>\n",
       "      <td>[tolong, untuk, pihak, xl, jika, update, aplik...</td>\n",
       "      <td>[tolong, xl, update, aplikasi, perbaiki, kemba...</td>\n",
       "      <td>tolong xl update aplikasi perbaiki kembangin b...</td>\n",
       "      <td>-4</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  tidak rekomen, signal hanya 1 bar,giliran lebi...   \n",
       "1  aduh makin paraha ajah sih ini makin kesini ma...   \n",
       "2  Aplikasi nya sering bermasalah jaringan selalu...   \n",
       "3  aplikasi sering error, harus login dengan nomo...   \n",
       "4  Tolong untuk pihak XL jika update aplikasi itu...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  tidak rekomen signal hanya bargiliran lebih da...   \n",
       "1  aduh makin paraha ajah sih ini makin kesini ma...   \n",
       "2  Aplikasi nya sering bermasalah jaringan selalu...   \n",
       "3  aplikasi sering error harus login dengan nomor...   \n",
       "4  Tolong untuk pihak XL jika update aplikasi itu...   \n",
       "\n",
       "                                text_casefoldingText  \\\n",
       "0  tidak rekomen signal hanya bargiliran lebih da...   \n",
       "1  aduh makin paraha ajah sih ini makin kesini ma...   \n",
       "2  aplikasi nya sering bermasalah jaringan selalu...   \n",
       "3  aplikasi sering error harus login dengan nomor...   \n",
       "4  tolong untuk pihak xl jika update aplikasi itu...   \n",
       "\n",
       "                                      text_normalize  \\\n",
       "0  tidak rekomen signal hanya bargiliran lebih da...   \n",
       "1  aduh makin paraha ajah sih ini makin kesini ma...   \n",
       "2  aplikasi nya sering bermasalah jaringan selalu...   \n",
       "3  aplikasi sering error harus login dengan nomor...   \n",
       "4  tolong untuk pihak xl jika update aplikasi itu...   \n",
       "\n",
       "                                     text_slangwords  \\\n",
       "0  tidak rekomen signal hanya bargiliran lebih da...   \n",
       "1  aduh makin paraha saja sih ini makin kesini ma...   \n",
       "2  aplikasi nya sering bermasalah jaringan selalu...   \n",
       "3  aplikasi sering error harus login dengan nomor...   \n",
       "4  tolong untuk pihak xl jika update aplikasi itu...   \n",
       "\n",
       "                                 text_tokenizingText  \\\n",
       "0  [tidak, rekomen, signal, hanya, bargiliran, le...   \n",
       "1  [aduh, makin, paraha, saja, sih, ini, makin, k...   \n",
       "2  [aplikasi, nya, sering, bermasalah, jaringan, ...   \n",
       "3  [aplikasi, sering, error, harus, login, dengan...   \n",
       "4  [tolong, untuk, pihak, xl, jika, update, aplik...   \n",
       "\n",
       "                                       text_stopword  \\\n",
       "0  [rekomen, signal, bargiliran, bar, koneksi, in...   \n",
       "1  [aduh, paraha, kesini, rekomen, signal, bar, g...   \n",
       "2  [aplikasi, bermasalah, jaringan, lag, bermain,...   \n",
       "3  [aplikasi, error, login, nomor, server, eror, ...   \n",
       "4  [tolong, xl, update, aplikasi, perbaiki, kemba...   \n",
       "\n",
       "                                          text_akhir  polarity_score  polarity  \n",
       "0  rekomen signal bargiliran bar koneksi internet...             -13  negative  \n",
       "1  aduh paraha kesini rekomen signal bar giliran ...             -17  negative  \n",
       "2  aplikasi bermasalah jaringan lag bermain game ...             -12  negative  \n",
       "3  aplikasi error login nomor server eror lihat n...             -19  negative  \n",
       "4  tolong xl update aplikasi perbaiki kembangin b...              -4  negative  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6e03e-0ad5-421f-8129-d068ddc17aca",
   "metadata": {},
   "source": [
    "## Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8dd0c83-0d4e-4e06-aea8-ed7b1701a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pisahkan data menjadi fitur (tweet) dan label (sentimen)\n",
    "X = df_clean['text_akhir']\n",
    "y = df_clean['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7501e331-c162-4647-80d3-40b3efdf1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstraksi fitur dengan TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=1000, min_df=20, max_df=0.8 )\n",
    "X_tfidf = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a59c03de-1b79-445d-896d-2ca04d925273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi sparse matrix menjadi array agar bisa diproses oleh SMOTE\n",
    "X_tfidf_array = X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "443cfa40-3676-40e1-8385-9408c1bb5a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\tuf gaming\\miniconda3\\envs\\main-da\\lib\\site-packages (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\tuf gaming\\miniconda3\\envs\\main-da\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\tuf gaming\\miniconda3\\envs\\main-da\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\tuf gaming\\miniconda3\\envs\\main-da\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tuf gaming\\miniconda3\\envs\\main-da\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tuf gaming\\miniconda3\\envs\\main-da\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TUF Gaming\\miniconda3\\envs\\main-da\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# 📌 Terapkan SMOTE untuk menyeimbangkan data\n",
    "smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_tfidf_array, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e224141-d987-4009-bed8-f91a385f5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat DataFrame dengan fitur yang benar\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=tfidf.get_feature_names_out())  # GUNAKAN tfidf\n",
    "df_resampled[\"label\"] = y_resampled  # Tambahkan label ke DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d68ce5b-5f02-4ba9-a592-07570795c7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "negative    13104\n",
      "positive    13104\n",
      "neutral     13104\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_resampled[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7738dae1-ee09-48c6-81fa-36d47cba8f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pisahkan fitur dan label dari data hasil resampling\n",
    "X = df_resampled.drop(\"label\", axis=1)\n",
    "y = df_resampled[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb0f9428-557b-4b16-9f59-4fceb9986405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagi data menjadi 80% training dan 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y,      # Agar distribusi label tetap seimbang\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a57be9-f948-4d9e-b39b-30aa8b5842af",
   "metadata": {},
   "source": [
    "## Model machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9345c502-7ccf-4595-aea7-9340cf5ed69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - accuracy_train: 0.9966612610893828\n",
      "Random Forest - accuracy_test: 0.9116113442706346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Membuat objek model Random Forest\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=800,         # Kurangi jumlah pohon\n",
    "    max_depth=None,             # Batasi kedalaman pohon\n",
    "    min_samples_split=10,      # Minimum sampel untuk split\n",
    "    min_samples_leaf=1,       # Minimum sampel di setiap leaf\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Melatih model Random Forest pada data pelatihan\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi sentimen pada data pelatihan dan data uji\n",
    "y_pred_train_rf = random_forest.predict(X_train)\n",
    "y_pred_test_rf = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluasi akurasi model Random Forest\n",
    "accuracy_train_rf = accuracy_score(y_pred_train_rf, y_train)\n",
    "accuracy_test_rf = accuracy_score(y_pred_test_rf, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Random Forest - accuracy_train:', accuracy_train_rf)\n",
    "print('Random Forest - accuracy_test:', accuracy_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "754a9ee9-1346-4dac-a4ea-85ae3e293818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - accuracy_train: 0.9481700531018474\n",
      "SVM - accuracy_test: 0.9235660689304337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Membuat objek model SVM\n",
    "svm_model = SVC(C=10, kernel='linear')  # Gunakan kernel linear untuk teks\n",
    "\n",
    "# Melatih model SVM pada data pelatihan\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi sentimen pada data pelatihan dan data uji\n",
    "y_pred_train_svm = svm_model.predict(X_train)\n",
    "y_pred_test_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluasi akurasi model SVM\n",
    "accuracy_train_svm = accuracy_score(y_pred_train_svm, y_train)\n",
    "accuracy_test_svm = accuracy_score(y_pred_test_svm, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('SVM - accuracy_train:', accuracy_train_svm)\n",
    "print('SVM - accuracy_test:', accuracy_test_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "180c2e25-7d75-4bd6-8bc3-2837486e2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagi data menjadi data latih 30% dan data uji\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9ab34f4-8ad2-487d-b462-87b0a9a1036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - accuracy_train: 0.9974925503306926\n",
      "Random Forest - accuracy_test: 0.9096998473800237\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=800,         # Kurangi jumlah pohon\n",
    "    max_depth=None,             # Batasi kedalaman pohon\n",
    "    min_samples_split=10,      # Minimum sampel untuk split\n",
    "    min_samples_leaf=1,       # Minimum sampel di setiap leaf\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Melatih model Random Forest pada data pelatihan\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi sentimen pada data pelatihan dan data uji\n",
    "y_pred_train_rf = random_forest.predict(X_train)\n",
    "y_pred_test_rf = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluasi akurasi model Random Forest\n",
    "accuracy_train_rf = accuracy_score(y_pred_train_rf, y_train)\n",
    "accuracy_test_rf = accuracy_score(y_pred_test_rf, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Random Forest - accuracy_train:', accuracy_train_rf)\n",
    "print('Random Forest - accuracy_test:', accuracy_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec8cab14-1089-4bd9-876e-e49bf05ff324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Masukkan kalimat baru:  aplikasinya jelek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentimen kalimat baru adalah NEGATIF.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TUF Gaming\\miniconda3\\envs\\main-da\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Input kalimat baru dari pengguna\n",
    "kalimat_baru = input(\"Masukkan kalimat baru: \")\n",
    "\n",
    "# Melakukan preprocessing pada kalimat baru\n",
    "kalimat_baru_cleaned = cleaningText(kalimat_baru)\n",
    "kalimat_baru_casefolded = casefoldingText(kalimat_baru_cleaned)\n",
    "kalimat_baru_slangfixed = fix_slangwords(kalimat_baru_casefolded)\n",
    "kalimat_baru_tokenized = tokenizingText(kalimat_baru_slangfixed)\n",
    "kalimat_baru_filtered = filteringText(kalimat_baru_tokenized)\n",
    "kalimat_baru_final = toSentence(kalimat_baru_filtered)\n",
    "\n",
    "# Menggunakan objek tfidf yang sudah di-fit dari pelatihan sebelumnya\n",
    "X_kalimat_baru = tfidf.transform([kalimat_baru_final]).toarray()\n",
    "\n",
    "# Memperoleh prediksi sentimen kalimat baru\n",
    "prediksi_sentimen = svm_model.predict(X_kalimat_baru)\n",
    "\n",
    "# Tampilkan hasil\n",
    "hasil = prediksi_sentimen[0]\n",
    "if hasil == 'positive':\n",
    "    print(\"Sentimen kalimat baru adalah POSITIF.\")\n",
    "elif hasil == 'neutral':\n",
    "    print(\"Sentimen kalimat baru adalah NETRAL.\")\n",
    "else:\n",
    "    print(\"Sentimen kalimat baru adalah NEGATIF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73db6ba-df6d-4851-9c37-0f8ec3d511dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
